{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ca1f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import torch\n",
    "from ollama import Client\n",
    "from tqdm import tqdm\n",
    "\n",
    "MODEL = \"gemma3:1b\"\n",
    "client = Client(host=\"http://localhost:11434\")\n",
    "\n",
    "embedding = torch.nn.Embedding(30, 768)\n",
    "embedding.load_state_dict(torch.load(\"softprompt_gemma1b.pt\"))\n",
    "embedding.eval()\n",
    "\n",
    "TEST_PATH = \"test_100.jsonl\"\n",
    "data = [json.loads(x) for x in open(TEST_PATH)]\n",
    "\n",
    "def build_prompt(q):\n",
    "    vec = \" \".join([f\"<v{i}>\" for i in range(30)])\n",
    "    return f\"{vec}\\nInstruction: {q}\"\n",
    "\n",
    "def extract(response):\n",
    "    m = re.search(r'\"p_answer\"\\s*:\\s*\"([^\"]+)\"', response)\n",
    "    return m.group(1).strip() if m else \"\"\n",
    "\n",
    "correct = 0\n",
    "for item in tqdm(data):\n",
    "    prompt = build_prompt(item[\"question\"])\n",
    "    result = client.generate(model=MODEL, prompt=prompt)\n",
    "    pred = extract(result[\"response\"]).replace(\" \", \"\")\n",
    "    \n",
    "    if pred == item[\"answer\"]:\n",
    "        correct += 1\n",
    "\n",
    "acc = correct / len(data) * 100\n",
    "print(f\"\\nðŸŽ¯ Evaluation Accuracy = {acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
